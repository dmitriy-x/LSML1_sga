{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0afc3f-7e1c-458d-bfc5-2be8edda3db8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyspark 3.5.3\n",
      "Uninstalling pyspark-3.5.3:\n",
      "  Successfully uninstalled pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e15d45-00a5-4880-ab85-56ac1c2dbd66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.2.4\n",
      "  Downloading pyspark-3.2.4.tar.gz (281.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m842.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.4-py2.py3-none-any.whl size=282040922 sha256=6af5e52f8069ffec131b5a05cf529be1cda1a750924383f4afc97270498cf7d1\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e7/e3/c8/c358dac750f2b6a4b03328d10e05a5c69501664bd6504b6c3e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Uninstalling py4j-0.10.9.7:\n",
      "      Successfully uninstalled py4j-0.10.9.7\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2cdbab5-edc7-488a-92c1-3a90a8d3f46b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.4\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56787199-7234-4ff5-a806-00b797fb7c6a",
   "metadata": {},
   "source": [
    " # Spark SQL Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f875d4e6-bdb2-478e-9edd-da6dd3c17154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"UserRoutes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d7f9518-7040-4fca-9510-3c40499f172a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"main\": 8184,\n",
      "    \"main-archive\": 1112,\n",
      "    \"main-rabota\": 1047,\n",
      "    \"main-internet\": 896,\n",
      "    \"main-bonus\": 870,\n",
      "    \"main-news\": 769,\n",
      "    \"main-tariffs\": 677,\n",
      "    \"main-online\": 587,\n",
      "    \"main-vklad\": 518,\n",
      "    \"main-rabota-archive\": 170\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"hdfs:/data/clickstream.csv\", inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "sql_query = \"\"\"\n",
    "WITH error_times AS (\n",
    "    SELECT user_id, session_id, MIN(timestamp) AS error_timestamp\n",
    "    FROM df\n",
    "    WHERE event_type LIKE '%error%'\n",
    "    GROUP BY user_id, session_id\n",
    "),\n",
    "filtered_events AS (\n",
    "    SELECT df.user_id, df.session_id, df.event_type, df.event_page, df.timestamp\n",
    "    FROM df\n",
    "    LEFT JOIN error_times ON df.user_id = error_times.user_id AND df.session_id = error_times.session_id\n",
    "    WHERE error_times.error_timestamp IS NULL OR df.timestamp < error_times.error_timestamp\n",
    "),\n",
    "page_events AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        event_page,\n",
    "        timestamp,\n",
    "        LAG(event_page) OVER (PARTITION BY user_id, session_id ORDER BY timestamp) AS prev_event_page\n",
    "    FROM filtered_events\n",
    "    WHERE event_type = 'page'\n",
    "),\n",
    "non_duplicate_pages AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        event_page,\n",
    "        timestamp\n",
    "    FROM page_events\n",
    "    WHERE event_page != prev_event_page OR prev_event_page IS NULL\n",
    "),\n",
    "routes_per_session AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        CONCAT_WS('-', TRANSFORM(\n",
    "            SORT_ARRAY(\n",
    "                COLLECT_LIST(NAMED_STRUCT('timestamp', timestamp, 'event_page', event_page))\n",
    "            ),\n",
    "            x -> x.event_page\n",
    "        )) AS route\n",
    "    FROM non_duplicate_pages\n",
    "    GROUP BY user_id, session_id\n",
    "),\n",
    "route_counts AS (\n",
    "    SELECT route, COUNT(*) AS count\n",
    "    FROM routes_per_session\n",
    "    GROUP BY route\n",
    ")\n",
    "SELECT route, count\n",
    "FROM route_counts\n",
    "ORDER BY count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(sql_query)\n",
    "top_routes = result_df.collect()\n",
    "\n",
    "# create a json, print and save to file\n",
    "result_dict = OrderedDict()\n",
    "for idx, row in enumerate(top_routes, start=1):    \n",
    "    result_dict[row['route']] = row['count']\n",
    "    \n",
    "print(json.dumps(result_dict, indent=4))\n",
    "\n",
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98f585-895d-452e-b874-3b89878f6545",
   "metadata": {},
   "source": [
    "# Spark DataFrame Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cac4b0ae-6e70-4049-b18d-509316a21b83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"main\": 8184,\n",
      "    \"main-archive\": 1112,\n",
      "    \"main-rabota\": 1047,\n",
      "    \"main-internet\": 896,\n",
      "    \"main-bonus\": 870,\n",
      "    \"main-news\": 769,\n",
      "    \"main-tariffs\": 677,\n",
      "    \"main-online\": 587,\n",
      "    \"main-vklad\": 518,\n",
      "    \"main-rabota-archive\": 170\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UserRoutes_DataFrame\").getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"hdfs:/data/clickstream.csv\", inferSchema=True)\n",
    "\n",
    "df = df.withColumn('is_error', df.event_type.contains('error'))\n",
    "\n",
    "error_times = df.filter(col('is_error')) \\\n",
    "    .groupBy('user_id', 'session_id') \\\n",
    "    .agg(min('timestamp').alias('error_timestamp'))\n",
    "\n",
    "df = df.join(error_times, on=['user_id', 'session_id'], how='left')\n",
    "\n",
    "df_filtered = df.filter(\n",
    "    col('error_timestamp').isNull() | (col('timestamp') < col('error_timestamp'))\n",
    ")\n",
    "\n",
    "page_events = df_filtered.filter(col('event_type') == 'page')\n",
    "\n",
    "window_spec = Window.partitionBy('user_id', 'session_id').orderBy('timestamp')\n",
    "\n",
    "page_events = page_events.withColumn(\n",
    "    'prev_event_page',\n",
    "    lag('event_page').over(window_spec)\n",
    ")\n",
    "\n",
    "non_duplicate_pages = page_events.filter(\n",
    "    (col('event_page') != col('prev_event_page')) | col('prev_event_page').isNull()\n",
    ")\n",
    "\n",
    "routes_df = non_duplicate_pages.groupBy('user_id', 'session_id') \\\n",
    "    .agg(\n",
    "        collect_list(\n",
    "            struct(col('timestamp'), col('event_page'))\n",
    "        ).alias('pages')\n",
    "    )\n",
    "\n",
    "routes_df = routes_df.withColumn(\n",
    "    'sorted_pages',\n",
    "    sort_array(col('pages'))\n",
    ").withColumn(\n",
    "    'route',\n",
    "    concat_ws(\n",
    "        '-',\n",
    "        expr(\"transform(sorted_pages, x -> x.event_page)\")\n",
    "    )\n",
    ")\n",
    "\n",
    "route_counts = routes_df.groupBy('route').agg(count('*').alias('count'))\n",
    "\n",
    "top_routes_df = route_counts.orderBy(desc('count')).limit(10)\n",
    "\n",
    "top_routes = top_routes_df.collect()\n",
    "\n",
    "# create a json, print and save to file\n",
    "result_dict = OrderedDict()\n",
    "for idx, row in enumerate(top_routes, start=1):    \n",
    "    result_dict[row['route']] = row['count']\n",
    "    \n",
    "print(json.dumps(result_dict, indent=4))\n",
    "\n",
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f39e5-b05a-4629-91ba-13324c7514d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark RDD Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e65289fb-76c7-4f6f-8b21-7575e672e7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"main\": 8184,\n",
      "    \"main-archive\": 1113,\n",
      "    \"main-rabota\": 1047,\n",
      "    \"main-internet\": 897,\n",
      "    \"main-bonus\": 870,\n",
      "    \"main-news\": 769,\n",
      "    \"main-tariffs\": 677,\n",
      "    \"main-online\": 587,\n",
      "    \"main-vklad\": 518,\n",
      "    \"main-rabota-archive\": 170\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import builtins  # Import builtins to access the built-in min function\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UserRoutes_RDD\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"hdfs:/data/clickstream.csv\", inferSchema=True)\n",
    "\n",
    "rdd = df.rdd\n",
    "\n",
    "def map_row(row):\n",
    "    user_id = row['user_id']\n",
    "    session_id = row['session_id']\n",
    "    event_type = row['event_type']\n",
    "    event_page = row['event_page']\n",
    "    timestamp = row['timestamp']\n",
    "    is_error = 'error' in event_type.lower()\n",
    "    return (user_id, session_id, event_type, event_page, timestamp, is_error)\n",
    "\n",
    "parsed_rdd = rdd.map(map_row)\n",
    "\n",
    "# Use builtins.min to ensure we're using the built-in min function\n",
    "error_timestamps = parsed_rdd.filter(lambda x: x[5]) \\\n",
    "    .map(lambda x: ((x[0], x[1]), x[4])) \\\n",
    "    .reduceByKey(builtins.min)\n",
    "\n",
    "session_events = parsed_rdd.map(lambda x: ((x[0], x[1]), x))\n",
    "\n",
    "joined_rdd = session_events.leftOuterJoin(error_timestamps)\n",
    "\n",
    "def filter_before_error(x):\n",
    "    ((user_id, session_id), (event, error_timestamp)) = x\n",
    "    timestamp = event[4]\n",
    "    if error_timestamp is None:\n",
    "        return True\n",
    "    return timestamp < error_timestamp\n",
    "\n",
    "filtered_rdd = joined_rdd.filter(filter_before_error)\n",
    "\n",
    "page_events = filtered_rdd.filter(lambda x: x[1][0][2] == 'page')\n",
    "\n",
    "def extract_pages(x):\n",
    "    ((user_id, session_id), (event, _)) = x\n",
    "    return ((user_id, session_id), [(event[4], event[3])])\n",
    "\n",
    "session_pages = page_events.map(extract_pages).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "def build_route(x):\n",
    "    pages = sorted(x[1], key=lambda y: y[0])\n",
    "    deduped_pages = []\n",
    "    last_page = None\n",
    "    for timestamp, page in pages:\n",
    "        if page != last_page:\n",
    "            deduped_pages.append(page)\n",
    "            last_page = page\n",
    "    route = '-'.join(deduped_pages)\n",
    "    return (route, 1)\n",
    "\n",
    "routes = session_pages.map(build_route)\n",
    "\n",
    "route_counts = routes.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "top_routes = route_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "# create a json, print and save to file\n",
    "result_dict = OrderedDict()\n",
    "for route_str, frequency in top_routes:\n",
    "    result_dict[route_str] = frequency\n",
    "    \n",
    "print(json.dumps(result_dict, indent=4))\n",
    "\n",
    "with open('result.json', 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b810b7-79f1-4a18-9bc9-fb417fa0bafc",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ececa281-b00a-418b-a116-197f8cca6eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "Correct main answer!\n",
      "Correct main-archive answer!\n",
      "Correct main-rabota answer!\n",
      "Correct main-internet answer!\n",
      "Correct main-bonus answer!\n",
      "Correct main-news answer!\n",
      "Correct main-tariffs answer!\n",
      "Correct main-online answer!\n",
      "Correct main-vklad answer!\n",
      "Correct main-rabota-archive answer!\n"
     ]
    }
   ],
   "source": [
    "#!curl -F file=@result.json 51.250.123.136:80/MDS-LSML1/<>/w6/2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
